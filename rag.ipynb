{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Employee_Handbook.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()\n",
    "pages = pages[4:]  # Skip the first few pages as they are not required\n",
    "text = \"\\n\".join([doc.page_content for doc in pages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split this text into smaller chunks to make it easier to handle in the chatbot\n",
    "# we use RecursiveCharacterTextSplitter to split the text into chunks of 500 characters each, with an overlap of 150 characters to ensure continuity.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "docs = text_splitter.create_documents([text])\n",
    "for i, d in enumerate(docs):\n",
    "    d.metadata = {\"doc_id\": i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724280654.591679 74289931 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": [
    "# Generating Embeddings\n",
    "# This function takes a a sentence as an arugument and return it's embeddings\n",
    "def get_embeddings(text):\n",
    "    # Define the embedding model\n",
    "    model = 'models/embedding-001'\n",
    "    # Get the embeddings\n",
    "    embedding = genai.embed_content(model=model,\n",
    "                                    content=text,\n",
    "                                    task_type=\"retrieval_document\")\n",
    "    return embedding['embedding']\n",
    "\n",
    "# Get the page_content from the documents and create a new list\n",
    "content_list = [doc.page_content for doc in docs]\n",
    "# Send one page_content at a time\n",
    "embeddings = [get_embeddings(content) for content in content_list]\n",
    "\n",
    "# Create a dataframe to ingest it to the database\n",
    "dataframe = pd.DataFrame({\n",
    "    'page_content': content_list,\n",
    "    'embeddings': embeddings\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing data in MyScaleDB\n",
    "import clickhouse_connect\n",
    "\n",
    "client = clickhouse_connect.get_client(\n",
    "    host='msc-fa078581.us-east-1.aws.myscale.com',\n",
    "    port=443,\n",
    "    username='jgajbha_org_default',\n",
    "    password='passwd_qz4OVgg71J0TrU'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/27 inserted.\n",
      "Batch 2/27 inserted.\n",
      "Batch 3/27 inserted.\n",
      "Batch 4/27 inserted.\n",
      "Batch 5/27 inserted.\n",
      "Batch 6/27 inserted.\n",
      "Batch 7/27 inserted.\n",
      "Batch 8/27 inserted.\n",
      "Batch 9/27 inserted.\n",
      "Batch 10/27 inserted.\n",
      "Batch 11/27 inserted.\n",
      "Batch 12/27 inserted.\n",
      "Batch 13/27 inserted.\n",
      "Batch 14/27 inserted.\n",
      "Batch 15/27 inserted.\n",
      "Batch 16/27 inserted.\n",
      "Batch 17/27 inserted.\n",
      "Batch 18/27 inserted.\n",
      "Batch 19/27 inserted.\n",
      "Batch 20/27 inserted.\n",
      "Batch 21/27 inserted.\n",
      "Batch 22/27 inserted.\n",
      "Batch 23/27 inserted.\n",
      "Batch 24/27 inserted.\n",
      "Batch 25/27 inserted.\n",
      "Batch 26/27 inserted.\n",
      "Batch 27/27 inserted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0', 'chi-msc-fa078581-msc-fa078581-0-0', 'OK', '0', '0']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table and insert the data\n",
    "# Create a table with the name 'handbook'\n",
    "client.command(\"\"\"\n",
    "    CREATE TABLE default.handbook (\n",
    "        id Int64,\n",
    "        page_content String,\n",
    "        embeddings Array(Float32),\n",
    "        CONSTRAINT check_data_length CHECK length(embeddings) = 768\n",
    "    ) ENGINE = MergeTree()\n",
    "    ORDER BY id\n",
    "\"\"\")\n",
    "\n",
    "# The CONSTRAINT will ensure that the length of each embedding vector is 768\n",
    "\n",
    "# Insert the data in batches\n",
    "batch_size = 10\n",
    "num_batches = len(dataframe) // batch_size\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch_data = dataframe[start_idx:end_idx]\n",
    "    # Insert the data\n",
    "    client.insert(\"default.handbook\", batch_data.to_records(index=False).tolist(), column_names=batch_data.columns.tolist())\n",
    "    print(f\"Batch {i+1}/{num_batches} inserted.\")\n",
    "# Create a vector index for a quick retrieval of data\n",
    "client.command(\"\"\"\n",
    "ALTER TABLE default.handbook\n",
    "    ADD VECTOR INDEX vector_index embeddings\n",
    "    TYPE MSTG\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving relevant docs\n",
    "def get_relevant_docs(user_query):\n",
    "    # Call the get_embeddings function again to convert user query into vector embeddngs\n",
    "    query_embeddings = get_embeddings(user_query)\n",
    "    # Make the query\n",
    "    results = client.query(f\"\"\"\n",
    "        SELECT page_content,\n",
    "        distance(embeddings, {query_embeddings}) as dist FROM default.handbook ORDER BY dist LIMIT 3\n",
    "    \"\"\")\n",
    "    relevant_docs = []\n",
    "    for row in results.named_results():\n",
    "        relevant_docs.append(row['page_content'])\n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, I cannot answer that question because the office working hours are not mentioned in the provided passage.\n"
     ]
    }
   ],
   "source": [
    "# generating a response\n",
    "# using the retrieved docs to generate a response to the user's query\n",
    "def make_rag_prompt(query, relevant_passage):\n",
    "    relevant_passage = ' '.join(relevant_passage)\n",
    "    prompt = (\n",
    "        f\"You are a helpful and informative chatbot that answers questions using text from the reference passage included below. \"\n",
    "        f\"Respond in a complete sentence and make sure that your response is easy to understand for everyone. \"\n",
    "        f\"Maintain a friendly and conversational tone. If the passage is irrelevant, feel free to ignore it.\\n\\n\"\n",
    "        f\"QUESTION: '{query}'\\n\"\n",
    "        f\"PASSAGE: '{relevant_passage}'\\n\\n\"\n",
    "        f\"ANSWER:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "def generate_response(user_prompt):\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    answer = model.generate_content(user_prompt)\n",
    "    return answer.text\n",
    "\n",
    "def generate_answer(query):\n",
    "    relevant_text = get_relevant_docs(query)\n",
    "    text = \" \".join(relevant_text)\n",
    "    prompt = make_rag_prompt(query, relevant_passage=relevant_text)\n",
    "    answer = generate_response(prompt)\n",
    "    return answer\n",
    "answer = generate_answer(query=\"what are the office working hours?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
